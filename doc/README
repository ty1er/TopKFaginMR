How to get the experimental results we got:

a. We have used the Cloudera CDH4 demo virtual machine, downloaded from the link on course project website.

b. Compile jar file:
    1. Install "Maven". Add maven "bin/" directory to PATH.
    2. Cd to project directory. Execute "mvn package" to build project.
    3. Step 2 creates a folder "target" with jar file "fagin.mapreduce-1.0-SNAPSHOT-job.jar".

*. Configuration Mapper and Reducer numbers. Cloudera claims that these numbers could be adjusted through argument at job runtime. However we failed to discover any impact from these arguments, therefore we have used static configuration, which requires source code modification.
   In FaginAlgorithm.java:
        1. To set the number of Mappers, use 
             sortingJob.setNumReduceTasks(2);
           in line 33.
        2. To set the number of Recuders, use
             topKFilterJob.setNumReduceTasks(2);
           in line 78.
        3. Recompile with Maven before running on hadoop.


c. Run jar on hadoop
    1. Create folders and copy dataset into hadoop 
    >> hadoop dfs -mkdir /user/cloudera/fagin
    >> hadoop dfs -mkdir /user/cloudera/fagin/input           [this is the input path]
    >> hadoop dfs -copyFromLocal dataset[1 or 2].txt /user/cloudera/fagin/input
    [make sure only ONE dataset file in /input before running, or you need to delete it before copy. >> hadoop dfs -rm /user/cloudera/fagin/input/* ]
    2. Run the jar
    Usage: hadoop jar [your jar file name].jar [input path] [output path] [topk number] [normalized = 1, not normalized = 0]
    >> hadoop jar fagin.mapreduce-1.0-SNAPSHOT-job.jar /user/cloudera/fagin/input /user/cloudera/fagin/output 5 0

d. Obtain top-k results and scores
    The results would be in /user/cloudera/fagin/output/result
    >> hadoop dfs -cat /user/cloudera/fagin/output/result/part-r-00000
    It will show you the top-k objects along with their scores. 
    [if you set the last parameter to 1, it is the normalized result, add 1.0 in each property, so subtract 5.0 from the last output.]

e. Check the execution time records
    The records are stored by the hadoop automatically in localhost
    In every round of MapReduce run [5 M/R in our Fagin Algoritm], it contains this information in the terminal:
        The url to track the job: http://localhost.localdomain:8088/proxy/application_[job-id]
        Running job: job_[job-id]
    To see the record, copy the link to the browser.
    The webpage has JobName, Elapsed time, Average Map time, Average Reduce time, which are shown in our report.

You can find the top 50 objects of two datasets in top50.txt.
If you have any problems about running the Fagin program, feel free to contact us.
Thank you!
